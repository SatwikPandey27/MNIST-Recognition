{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"sRupojq_Y2OF"},"outputs":[],"source":["i=1234\n","rev=0\n","while (int(i)>0):\n","    temp=int(i)%10\n","    rev=((rev)*10)+(temp)\n","    i/=10\n","print (rev)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hl825niuY2OJ","collapsed":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import matplotlib\n","import matplotlib.pyplot as plot\n","%matplotlib inline\n","from sklearn.datasets import fetch_openml\n","mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n","mnist.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DYlMIvUwY2OK"},"outputs":[],"source":["X,y= mnist[\"data\"], mnist[\"target\"]\n","X.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QVM-neFjY2OL"},"outputs":[],"source":["y.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5adzZt3OY2OL"},"outputs":[],"source":["import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","some_digit = X[0]\n","some_digit_image = some_digit.reshape(28, 28)\n","plt.imshow(some_digit_image, cmap = mpl.cm.binary, interpolation=\"nearest\")\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BkBng4ajY2OM"},"outputs":[],"source":["y[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vpTJ4ws7Y2OM"},"outputs":[],"source":["y=y.astype(np.uint8)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ICEYRWhY2ON"},"outputs":[],"source":["X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"]},{"cell_type":"markdown","metadata":{"id":"SFkEs2YHY2OO"},"source":["## Training a Binary Classifier\n","This binary classifier is only for one digit, lets take '5' here it works as follows :\n","\n","-True if '5'\n","\n","-False if 'not 5'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6XGPG8IOY2OP"},"outputs":[],"source":["y_train_5 = (y_train == 5)\n","y_test_5 = (y_test == 5) #True for if the digit is 5, false otherwise."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nUkGdDSeY2OQ"},"outputs":[],"source":["from sklearn.linear_model import SGDClassifier\n","sgd_clf=SGDClassifier(random_state=42)\n","sgd_clf.fit(X_train, y_train_5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dwuwVjFCY2OR"},"outputs":[],"source":["sgd_clf.predict([some_digit])"]},{"cell_type":"markdown","metadata":{"id":"AR22Pj1NY2OR"},"source":["#### Taking some performance measures\n","Here we will be using cross-validation.\n","\n","Here we have 3 folds that means that we will train the model on 2 of the folds and test it on the third (each time)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YEB0L-KsY2OS"},"outputs":[],"source":["from sklearn.model_selection import cross_val_score\n","cross_val_score(sgd_clf , X_train , y_train_5 , cv=3 , scoring=\"accuracy\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jKlL6T_UY2OS"},"outputs":[],"source":["from sklearn.base import BaseEstimator\n","class Never5Classifier(BaseEstimator):\n","    def fit(self, X, y=None):\n","        pass\n","    def predict(self, X):\n","        return np.zeros((len(X), 1), dtype=bool)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CvVf9AJMY2OT"},"outputs":[],"source":["never_5_clf = Never5Classifier()\n","cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")"]},{"cell_type":"markdown","metadata":{"id":"WEgiEn6oY2OT"},"source":["It has over 90% accuracy! This is simply because only about 10% of the\n","images are 5s, so if you always guess that an image is not a 5, you will be right about\n","90% of the time.\n","This demonstrates why accuracy is generally not the preferred performance measure\n","for classifiers, especially when you are dealing with skewed datasets (i.e., when some\n","classes are much more frequent than others).\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jkv9DhhwY2OU"},"source":["A better method to evaluate the performance of a classifier is to look at the confusion matrix. The general idea is to count the number of times the instance of class 'A' were classified as objects of class 'B'.\n","\n","For example how many times were the images of digit 5 confused with the images of digit 3.\n","\n","So, for this first we need to make some predictions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5AGjRTm1Y2OU"},"outputs":[],"source":["#here we will use cross_val_predict() function:\n","\n","from sklearn.model_selection import cross_val_predict\n","y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\n","\n","#just like cross_val_score() function, cross_val_predict performs K-fold cross-validation,\n","#but instead of returning the evaluation scores, it returns the predictions made on each test fold."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nruxh7OBY2OU"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix\n","confusion_matrix (y_train_5, y_train_pred)"]},{"cell_type":"markdown","metadata":{"id":"qW1OQfEkY2OV"},"source":["Each row in a confusion matrix represents an actual class and each column represents a predicted class.\n","but, a perfect classifier would have only true positives and true negatives."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sjWBxA-oY2OV"},"outputs":[],"source":["y_train_perfect_predictions=y_train_5\n","confusion_matrix(y_train_5, y_train_perfect_predictions)"]},{"cell_type":"markdown","metadata":{"id":"g5-3WQLHY2OV"},"source":["Precision and Recall:\n","\n","The confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric. An interesting one to lo0ok at is the accuracy of the positive predictions, also called 'PRECISION'.\n","\n","precision = TP / TP+FP\n","\n","A trivial way to have perfect precision is to make one single positive prediction and ensure it is correct (precision = 1/1 = 100%). This would not be very useful since the classifier would ignore all but one positive instance. So precision is typically used along with another metric named recall, also called sensitivity or true positive rate.\n","\n","recall= TP / TP+FN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KL1ncIXsY2Oa"},"outputs":[],"source":["#Training a randomforestclassifier and comparing its results to our SGDclassifier\n","\n","from sklearn.ensemble import RandomForestClassifier\n","\n","forest_clf = RandomForestClassifier(random_state=42)\n","y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3, method=\"predict_proba\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_CMK5PcuY2Oa"},"outputs":[],"source":["# But to plot a ROC curve, you need scores, not probabilities.\n","# A simple solution is to use the positive class’s probability as the score:\n","\n","y_scores_forest = y_probas_forest[:, 1] # score = proba of positive class\n","fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5,y_scores_forest)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XXqO58BiY2Ob"},"outputs":[],"source":["plt.plot(fpr, tpr, \"b:\", label=\"SGD\")\n","plot_roc_curve(fpr_forest, tpr_forest, \"Random Forest\")\n","plt.legend(loc=\"lower right\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FBdoTBfHY2Ob"},"outputs":[],"source":["roc_auc_score(y_train_5, y_scores)\n","#area under curve for sgd classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UN-GfeA_Y2Ob"},"outputs":[],"source":["roc_auc_score(y_train_5, y_scores_forest)\n","#are under curve for random forest classifier"]},{"cell_type":"markdown","metadata":{"id":"YVNiCKNYY2Ob"},"source":["From the above scores it can be easily concluded that the random forest is a much better classifier than the sgd classifier in tha case of binary classifiers."]},{"cell_type":"markdown","metadata":{"id":"qwmSbwiXY2Ob"},"source":["## Multiclass Classification\n","\n","Multiclass Classifiers also known as multinomial classifiers can distinguish between more than two classes.\n","\n","There are two strategies followed to solve multiclass classification problems:\n","1. Create a system that can classify the digit images into 10\n","classes (from 0 to 9) is to train 10 binary classifiers, one for each digit (a 0-detector, a\n","1-detector, a 2-detector, and so on). Then when you want to classify an image, you get\n","the decision score from each classifier for that image and you select the class whose\n","classifier outputs the highest score. This is called the one-versus-all (OvA) strategy \n","(also called one-versus-the-rest).\n","\n","2. train a binary classifier for every pair of digits: one to distin‐\n","guish 0s and 1s, another to distinguish 0s and 2s, another for 1s and 2s, and so on.\n","This is called the one-versus-one (OvO) strategy. If there are N classes, you need to\n","train N × (N – 1) / 2 classifiers. For the MNIST problem, this means training 45\n","binary classifiers! When you want to classify an image, you have to run the image\n","through all 45 classifiers and see which class wins the most duels. The main advan‐\n","tage of OvO is that each classifier only needs to be trained on the part of the training\n","set for the two classes that it must distinguish"]},{"cell_type":"markdown","metadata":{"id":"LCLofMx1Y2Ob"},"source":["Some algorithms (such as Support Vector Machine classifiers) scale poorly with the\n","size of the training set, so for these algorithms OvO is preferred since it is faster to\n","train many classifiers on small training sets than training few classifiers on large\n","training sets. For most binary classification algorithms, however, OvA is preferred."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R3u8-f_aY2Oc"},"outputs":[],"source":["sgd_clf.fit(X_train, y_train)\n","sgd_clf.predict([some_digit])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kn9UECcCY2Oc"},"outputs":[],"source":["some_digit_scores=sgd_clf.decision_function([some_digit])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DzG2UNhdY2Oc"},"outputs":[],"source":["some_digit_scores"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IQ6iWtf-Y2Oc"},"outputs":[],"source":["np.argmax(some_digit_scores)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qzhi8wCqY2Oc"},"outputs":[],"source":["sgd_clf.classes_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q3gGcLfUY2Od"},"outputs":[],"source":["sgd_clf.classes_[5]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gJBwm9rCY2Od"},"outputs":[],"source":["from sklearn.svm import SVC\n","\n","svm_clf = SVC(gamma=\"auto\", random_state=42)\n","svm_clf.fit(X_train[:1000], y_train[:1000]) # y_train, not y_train_5\n","svm_clf.predict([some_digit])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jIdOCGYoY2Od"},"outputs":[],"source":["some_digit_scores = svm_clf.decision_function([some_digit])\n","some_digit_scores"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6sEDll0eY2Od"},"outputs":[],"source":["np.argmax(some_digit_scores)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qLc8JI_1Y2Od"},"outputs":[],"source":["svm_clf.classes_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IRa9s7KIY2Oe"},"outputs":[],"source":["svm_clf.classes_[5]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bhUM-OXiY2Oe"},"outputs":[],"source":["#Training a random forest classifier:\n","forest_clf.fit(X_train, y_train)\n","forest_clf.predict([some_digit])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kEH-G_vwY2Oe"},"outputs":[],"source":["forest_clf.predict_proba([some_digit])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s9mqQsqxY2Of"},"outputs":[],"source":["cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Z0BXNWxY2Of"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","X_train_scaled=scaler.fit_transform(X_train.astype(np.float64))\n","cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")"]},{"cell_type":"markdown","metadata":{"id":"6820YSj6Y2Of"},"source":["*Cross validation:\n","\n","cross validation allows us to compare different machine learning methods and see how well they perform in practice"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c8n441kDY2Of"},"outputs":[],"source":["#Excercise Question\n","from sklearn.neighbors import KNeighborsClassifier\n","knn=KNeighborsClassifier(n_neighbors=4)\n","knn.fit(X_train_scaled, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nsk_a0XWY2Of"},"outputs":[],"source":["cross_val_score(knn, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y6gdQQYcY2Og"},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","\n","param_grid = [{'weights': [\"uniform\", \"distance\"], 'n_neighbors': [3, 4, 5]}]\n","\n","knn = KNeighborsClassifier()\n","grid_search = GridSearchCV(knn, param_grid, cv=5, verbose=3)\n","grid_search.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"paoSyviYY2Og"},"outputs":[],"source":["grid_search.best_params_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YW-DHymWY2Og"},"outputs":[],"source":["grid_search.best_score_"]},{"cell_type":"markdown","metadata":{"id":"X7YrPw1mY2Oi"},"source":["## MULTI LABEL CLASSIFIERS\n","Multi-label classification involves predicting zero or more class labels. Unlike normal classification tasks where class labels are mutually exclusive. \n","\n","For example if you train a classifier to recognize three faces, Alice, Bob, and Charlie; then\n","when it is shown a picture of Alice and Charlie, it should output [1, 0, 1] (meaning\n","“Alice yes, Bob no, Charlie yes”)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-EorPheoY2Oi"},"outputs":[],"source":["from sklearn.neighbors import KNeighborsClassifier\n","\n","y_train_large = (y_train >= 7)\n","y_train_odd = (y_train % 2 == 1)\n","y_multilabel = np.c_[y_train_large, y_train_odd]\n","\n","knn_clf = KNeighborsClassifier()\n","knn_clf.fit(X_train, y_multilabel)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gPFxoX9BY2Oi"},"outputs":[],"source":["knn_clf.predict([some_digit])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2NV-mmeHY2Oi"},"outputs":[],"source":["y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)\n","f1_score(y_multilabel, y_train_knn_pred, average=\"macro\")"]},{"cell_type":"markdown","metadata":{"id":"EMxb-vR7Y2Oj"},"source":["## Finally testing the model on Test Set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3AFrPBCiY2Oj"},"outputs":[],"source":["from sklearn.neighbors import KNeighborsClassifier\n","knn=KNeighborsClassifier(n_neighbors=4, weights='distance')\n","knn.fit(X_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vqJrp_fiY2Oj"},"outputs":[],"source":["cross_val_score(knn, X_test, y_test, cv=3, scoring=\"accuracy\")"]},{"cell_type":"markdown","metadata":{"id":"QSLx2Kr3Y2Ok"},"source":["normal output : array([0.91241752, 0.93939394, 0.95439544])\n","weights = distance output : array([0.91841632, 0.9429943 , 0.96189619])\n","weights = uniform output : array([0.91241752, 0.93939394, 0.95439544])"]},{"cell_type":"markdown","metadata":{"id":"BLFqWpkPY2Ok"},"source":["The knn algorithm gives about 96% accurate classifications on our test set"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.0 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"4eacae1e01e22963f22cfce9930dbf670d814c1891345503b4533cebadfe6f69"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}